{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_DSM2_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMkP2vu50jDG7k53UxA8LQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QiSiyu/Delta-Modelling-ANN/blob/main/Train_DSM2_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UURVNFH6oSEZ"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Vc406RFjMu"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoF2IDzuFbBJ",
        "outputId": "23090219-4925-4147-9a31-4b5abd0e742f"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import sys\n",
        "import functools\n",
        "from pathlib import Path\n",
        "\n",
        "google_drive_dir = 'Delta-Modelling-ANN'\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir,'src'))\n",
        "\n",
        "data_resolution = '1D' # '1D' : daily; '15min' : 15-minute\n",
        "key_stations = ['RSAN018', 'RSAC092', 'CHSWP003', 'CHDMC006', 'SLMZU025', 'ROLD059', 'CHVT000']\n",
        "\n",
        "# daily data: <= 7306 samples, with 1282 input variables & 25 stations\n",
        "# 15-min data: TODO\n",
        "\n",
        "train_key_station_only = True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1zWOWv9oQL4"
      },
      "source": [
        "## Define hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq0BzwnoFlR7"
      },
      "source": [
        "epochs = 1\n",
        "batch_size = 100\n",
        "initial_lr = 0.01\n",
        "num_daily_values = 8\n",
        "avg_window_size = 11\n",
        "num_windows = 10\n",
        "training_set_ratio = 0.8\n",
        "\n",
        "window_size = num_windows * avg_window_size + num_daily_values\n",
        "\n",
        "# True: convolutional layer is trainable; False: conv layer is fixed\n",
        "train_conv = True\n",
        "\n",
        "\n",
        "# create a folder to save trained models\n",
        "model_folder = 'saved_models'\n",
        "\n",
        "Path(os.path.join('/content/drive','My Drive',google_drive_dir,model_folder)).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUkIlVbqFnNF"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEX9b1y8FodS",
        "outputId": "e78d4f6e-948e-4eb3-d422-46d2c2697765"
      },
      "source": [
        "data_files=sorted(tf.io.gfile.glob(os.path.join('/content/drive','My Drive',google_drive_dir,'src','data_%s_*.csv')%data_resolution))\n",
        "\n",
        "fline=open(data_files[0]).readline().rstrip() # Reading column names \n",
        "variables=fline.split(',')\n",
        "\n",
        "input_variables = [var for var in variables if 'input' in var]\n",
        "output_variables = [var for var in variables if 'target' in var]\n",
        "selected_input_variables = input_variables[0:]\n",
        "\n",
        "\n",
        "DATASET_SIZE = -len(data_files) # number reserved for headers\n",
        "\n",
        "for data_file in data_files:\n",
        "    for row in open(data_file):\n",
        "        DATASET_SIZE += 1\n",
        "train_size = int(0.8 * DATASET_SIZE)\n",
        "print('%d samples, each has %d input variable(s) & %d output station(s)' % (DATASET_SIZE, len(input_variables),len(output_variables)))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7306 samples, each has 1282 input variable(s) & 25 output station(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHwR0xcdFsxG"
      },
      "source": [
        "## Data preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd5pDp55F0iO"
      },
      "source": [
        "def pack_features_vector(features,ec):\n",
        "    \"\"\"Pack the features into a single array.\"\"\"\n",
        "    features = tf.stack([tf.cast(x,tf.float32) for x in list(features.values())], axis=1)\n",
        "    if ec.dtype==tf.float32:\n",
        "        return features, ec\n",
        "    else:\n",
        "        return features, tf.strings.to_number(ec) if not tf.reduce_any(ec== b'') else tf.convert_to_tensor([float('NaN')],dtype=tf.float32)\n",
        "\n",
        "def apply_window(dataset, window_size, batch_size,training_set_ratio=0.8):\n",
        "    windowed_dataset = dataset.map(pack_features_vector)\n",
        "    windowed_dataset = windowed_dataset.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices((x, y)))\n",
        "    filter_nan_in_ec = lambda x, y: not tf.reduce_any(tf.math.logical_or(tf.math.is_nan(y),tf.math.less_equal(y,tf.constant([0],dtype=tf.float32))))\n",
        "    windowed_dataset = windowed_dataset.filter(filter_nan_in_ec)\n",
        "\n",
        "    num_samples = 0\n",
        "    for _ in windowed_dataset:\n",
        "        num_samples += 1\n",
        "    train_size = int(training_set_ratio*num_samples)\n",
        "    if num_samples - train_size < window_size:\n",
        "        print('Available dataset size (%d training, %d test) smaller than window size (%d), will skip' % (train_size, num_samples - train_size,window_size))\n",
        "        return None, None, 0, 0\n",
        "    # else:\n",
        "    #     print('Available data samples: %d training, %d test' % (train_size, num_samples - train_size))\n",
        "\n",
        "    windowed_dataset = windowed_dataset.window(window_size, shift=1, drop_remainder=True)\n",
        "    windowed_dataset = windowed_dataset.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(window_size), y)))\n",
        " \n",
        "    windowed_trainset = windowed_dataset.take(train_size)\n",
        "    windowed_testset = windowed_dataset.skip(train_size)\n",
        "\n",
        "    return windowed_trainset.batch(batch_size).repeat(),\\\n",
        "        windowed_testset.batch(batch_size).repeat(),\\\n",
        "        train_size,\\\n",
        "        num_samples-train_size"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CX1V_LHDAXi"
      },
      "source": [
        "def load_and_window_dataset(data_files, input_variables, output_variable, window_size,batch_size,training_set_ratio=0.8):\n",
        "    assert len(output_variable)==1, 'This script is for single-station estimation!'\n",
        "\n",
        "    # 1282 input variables, 25 salinity (output) stations\n",
        "    ec_csv_ds = tf.data.experimental.make_csv_dataset(\n",
        "        data_files,\n",
        "        batch_size=1,\n",
        "        na_value='-2.000', # missing entries in output\n",
        "        select_columns=input_variables+output_variable,\n",
        "        label_name=output_variable[0],# work on first output station\n",
        "        num_epochs=1,\n",
        "        ignore_errors=False,\n",
        "        shuffle=False)\n",
        "\n",
        "    windowed_trainset,windowed_testset,train_size,test_size = apply_window(ec_csv_ds, window_size, batch_size,training_set_ratio)\n",
        "\n",
        "    #     for feature, label in windowed_testset.take(1):\n",
        "    #         print('%s windowed feature shape: ' % (output_variable[0]), feature.shape, 'Label shape: ', label.shape)\n",
        "\n",
        "    return windowed_trainset, windowed_testset, train_size,test_size\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IxXjdKtHvon"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZMKHa5laLDq"
      },
      "source": [
        "# function that generates manual weights for first conv layer\n",
        "def conv_filter_generator(num_daily_values=7,avg_window_size = 11, num_windows=10):\n",
        "    w = np.zeros((1,num_daily_values+num_windows*avg_window_size,num_daily_values+num_windows))\n",
        "    for ii in range(num_daily_values):\n",
        "        w[0,num_daily_values+num_windows*avg_window_size-ii-1,num_daily_values-ii-1] = 1\n",
        "    for ii in range(num_windows):\n",
        "        w[0,((num_windows-ii-1)*avg_window_size):((num_windows-ii)*avg_window_size),num_daily_values+ii] = 1/avg_window_size\n",
        "    return w\n",
        "\n",
        "# weights to initialize the first conv layer\n",
        "conv_filter_init = tf.keras.initializers.Constant(conv_filter_generator(num_daily_values=num_daily_values,\n",
        "                                                                avg_window_size=avg_window_size,\n",
        "                                                                num_windows=num_windows))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4w6fZQjHtip"
      },
      "source": [
        "def build_ann(input_shape,num_daily_values, num_windows, conv_filter_init=None,train_conv=True,print_details=False):\n",
        "    conv_layer = tf.keras.layers.Conv1D(num_daily_values+num_windows, 1, activation=None,\n",
        "                              kernel_initializer=conv_filter_init,\n",
        "                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0))\n",
        "    conv_layer.trainable=train_conv\n",
        "    \n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Permute((2,1))(inputs)\n",
        "    x = tf.keras.layers.Masking(mask_value=-1)(x)\n",
        "    x = conv_layer(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(10, activation=\"elu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(10, activation=\"elu\")(x)\n",
        "    # x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "    outputs = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"ann_model\")\n",
        "    if print_details:\n",
        "        model.summary()\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rCYZvpxFDXY"
      },
      "source": [
        "## Define Callback functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni3OtFlyFE7G"
      },
      "source": [
        "### Learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kjaz9UFLAY"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 40%, 60%, 80%, 90% of total epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = initial_lr\n",
        "    if epoch > 0.9*epochs:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 0.8*epochs:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 0.6*epochs:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 0.4*epochs:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "  \n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxdt7MloFY2C"
      },
      "source": [
        "### Learning rate reducer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSL3JW-BFUbD"
      },
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                                                 cooldown=0,\n",
        "                                                 patience=5,\n",
        "                                                 min_lr=initial_lr/100)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdA8tViTF_Fu"
      },
      "source": [
        "### Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRoIW3Z8GCKi"
      },
      "source": [
        "def mse_loss_masked(y_true, y_pred):\n",
        "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred,y_true))\n",
        "    return squared_diff/(tf.reduce_sum(tf.cast(y_true>0, tf.float32))+0.01)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cOyl8r7HxdP"
      },
      "source": [
        "# Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1UjfI3sIXHD",
        "outputId": "3b39068c-b736-431d-8d2b-e76069ac15c6"
      },
      "source": [
        "losses = {}\n",
        "val_losses = {}\n",
        "ann_number = 0\n",
        "for ii in range(len(output_variables)):\n",
        "    if train_key_station_only and (output_variables[ii].replace('target','') not in key_stations):\n",
        "        # if training for key stations only, skip non-key stations\n",
        "        continue\n",
        "    ann_number += 1\n",
        "    print('Training ANN #%d for %s Station %s' % (ann_number, 'key' if train_key_station_only else '', output_variables[ii].replace('target','')))\n",
        "    \n",
        "    path_checkpoint = os.path.join('/content/drive','My Drive',google_drive_dir,model_folder,\"%s_checkpoint.h5\" % (output_variables[ii].replace('target','') + ('_train_conv' if train_conv else '_freeze_conv')))\n",
        "    # es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
        "    \n",
        "    modelckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        filepath=path_checkpoint,\n",
        "        verbose=1,\n",
        "        save_weights_only=False,\n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    # prepare dataset\n",
        "    selected_output_variable = output_variables[ii] # one output station at a time\n",
        "    windowed_trainset, windowed_testset, train_size, test_size = load_and_window_dataset(data_files, \n",
        "                                                                                         selected_input_variables,\n",
        "                                                                                         [selected_output_variable],\n",
        "                                                                                         window_size,\n",
        "                                                                                         batch_size,\n",
        "                                                                                         training_set_ratio)\n",
        "    if train_size == 0 or test_size == 0:\n",
        "        print('No data available, stop training.')\n",
        "        continue\n",
        "\n",
        "    # build new model\n",
        "    model = build_ann(input_shape=(window_size, len(selected_input_variables)),\n",
        "                      num_daily_values=num_daily_values,\n",
        "                      num_windows=num_windows,\n",
        "                      conv_filter_init=conv_filter_init,\n",
        "                      train_conv=train_conv)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "    \n",
        "    model.compile(\n",
        "        loss=mse_loss_masked,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "    history = model.fit(\n",
        "      windowed_trainset,\n",
        "      epochs=epochs,\n",
        "      steps_per_epoch=train_size//batch_size,\n",
        "      validation_data=windowed_testset,\n",
        "      validation_freq=1,\n",
        "      validation_steps=test_size//batch_size,\n",
        "      callbacks=[lr_scheduler,modelckpt_callback],\n",
        "      verbose=1\n",
        "    )\n",
        "    losses[selected_output_variable] = history.history['loss'][-1]\n",
        "    val_losses[selected_output_variable] = history.history['val_loss'][-1]\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ANN #1 for key Station CHDMC006\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.0710 - mean_squared_error: 0.0710\n",
            "Epoch 00001: val_loss improved from inf to 0.12614, saving model to /content/drive/My Drive/Delta-Modelling-ANN/saved_models/CHDMC006_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 67s 639ms/step - loss: 0.0710 - mean_squared_error: 0.0710 - val_loss: 0.1261 - val_mean_squared_error: 0.1261 - lr: 0.0100\n",
            "Training ANN #2 for key Station CHSWP003\n",
            "Learning rate:  0.01\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.2017 - mean_squared_error: 0.2017\n",
            "Epoch 00001: val_loss improved from inf to 0.44983, saving model to /content/drive/My Drive/Delta-Modelling-ANN/saved_models/CHSWP003_train_conv_checkpoint.h5\n",
            "54/54 [==============================] - 37s 658ms/step - loss: 0.2017 - mean_squared_error: 0.2017 - val_loss: 0.4498 - val_mean_squared_error: 0.4499 - lr: 0.0100\n",
            "Training ANN #3 for key Station ROLD059\n",
            "Learning rate:  0.01\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.2474 - mean_squared_error: 0.2474\n",
            "Epoch 00001: val_loss improved from inf to 3.96580, saving model to /content/drive/My Drive/Delta-Modelling-ANN/saved_models/ROLD059_train_conv_checkpoint.h5\n",
            "43/43 [==============================] - 37s 755ms/step - loss: 0.2474 - mean_squared_error: 0.2474 - val_loss: 3.9658 - val_mean_squared_error: 3.9662 - lr: 0.0100\n",
            "Training ANN #4 for key Station RSAC092\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
            "Epoch 00001: val_loss improved from inf to 0.04233, saving model to /content/drive/My Drive/Delta-Modelling-ANN/saved_models/RSAC092_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 39s 658ms/step - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0423 - val_mean_squared_error: 0.0423 - lr: 0.0100\n",
            "Training ANN #5 for key Station RSAN018\n",
            "Learning rate:  0.01\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
            "Epoch 00001: val_loss improved from inf to 0.07666, saving model to /content/drive/My Drive/Delta-Modelling-ANN/saved_models/RSAN018_train_conv_checkpoint.h5\n",
            "58/58 [==============================] - 39s 651ms/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0767 - val_mean_squared_error: 0.0767 - lr: 0.0100\n",
            "Training ANN #6 for key Station SLMZU025\n",
            "Learning rate:  0.01\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1234 - mean_squared_error: 0.1234\n",
            "Epoch 00001: val_loss improved from inf to 0.10984, saving model to /content/drive/My Drive/Delta-Modelling-ANN/saved_models/SLMZU025_train_conv_checkpoint.h5\n",
            "42/42 [==============================] - 37s 764ms/step - loss: 0.1234 - mean_squared_error: 0.1234 - val_loss: 0.1098 - val_mean_squared_error: 0.1099 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_n-mdjPTHPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a517dab-51e6-49af-ff5f-4bbe93c1357b"
      },
      "source": [
        "print ((\"{:<12}\" * 3).format('', 'Loss', 'Val Loss'))\n",
        "\n",
        "for station, loss in losses.items():\n",
        "    print((\"{:<12}\" * 3).format(station.replace('target',''), np.round(loss,8), np.round(val_losses[station],8)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Loss        Val Loss    \n",
            "CHDMC006    0.07102563  0.12613571  \n",
            "CHSWP003    0.20171529  0.4498325   \n",
            "ROLD059     0.24739309  3.96579671  \n",
            "RSAC092     0.03662964  0.04232566  \n",
            "RSAN018     0.0338104   0.07665597  \n",
            "SLMZU025    0.12335092  0.10984158  \n"
          ]
        }
      ]
    }
  ]
}