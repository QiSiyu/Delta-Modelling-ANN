{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_DSM2_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvcbacIVmLzry53aNJlMB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QiSiyu/Delta-Modelling-ANN/blob/main/Train_DSM2_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UURVNFH6oSEZ"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Vc406RFjMu"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoF2IDzuFbBJ",
        "outputId": "ecad5969-ceb8-4186-e2b3-ffb721091ee9"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import sys\n",
        "import functools\n",
        "from pathlib import Path\n",
        "\n",
        "google_drive_dir = 'DeltaModelling'\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "\n",
        "data_resolution = '1D' # '1D' : daily; '15min' : 15-minute\n",
        "key_stations = ['RSAN018', 'RSAC092', 'CHSWP003', 'CHDMC006', 'SLMZU025', 'ROLD059', 'CHVT000']\n",
        "\n",
        "\n",
        "# daily data: <= 7306 samples, with 1282 input variables & 25 stations\n",
        "# 15-min data: TODO"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1zWOWv9oQL4"
      },
      "source": [
        "## Define hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq0BzwnoFlR7"
      },
      "source": [
        "epochs = 1\n",
        "batch_size = 100\n",
        "initial_lr = 0.01\n",
        "num_daily_values = 8\n",
        "avg_window_size = 11\n",
        "num_windows = 10\n",
        "training_set_ratio = 0.8\n",
        "\n",
        "window_size = num_windows * avg_window_size + num_daily_values\n",
        "\n",
        "# True: convolutional layer is trainable; False: conv layer is fixed\n",
        "train_conv = True\n",
        "\n",
        "\n",
        "# create a folder to save trained models\n",
        "model_folder = 'saved_models'\n",
        "\n",
        "Path(os.path.join('/content/drive','My Drive',google_drive_dir,model_folder)).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUkIlVbqFnNF"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEX9b1y8FodS",
        "outputId": "acc1bdba-82b6-4000-e577-b4c7572e5d93"
      },
      "source": [
        "data_files=sorted(tf.io.gfile.glob(os.path.join('/content/drive','My Drive',google_drive_dir,'data_%s_*.csv')%data_resolution))\n",
        "\n",
        "fline=open(data_files[0]).readline().rstrip() # Reading column names \n",
        "variables=fline.split(',')\n",
        "\n",
        "input_variables = [var for var in variables if 'input' in var]\n",
        "output_variables = [var for var in variables if 'target' in var]\n",
        "selected_input_variables = input_variables[0:]\n",
        "\n",
        "\n",
        "DATASET_SIZE = -len(data_files) # number reserved for headers\n",
        "\n",
        "for data_file in data_files:\n",
        "    for row in open(data_file):\n",
        "        DATASET_SIZE += 1\n",
        "train_size = int(0.8 * DATASET_SIZE)\n",
        "print('%d samples, each has %d input variable(s) & %d output station(s)' % (DATASET_SIZE, len(input_variables),len(output_variables)))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7306 samples, each has 1282 input variable(s) & 25 output station(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHwR0xcdFsxG"
      },
      "source": [
        "## Data preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd5pDp55F0iO"
      },
      "source": [
        "def pack_features_vector(features,ec):\n",
        "    \"\"\"Pack the features into a single array.\"\"\"\n",
        "    features = tf.stack([tf.cast(x,tf.float32) for x in list(features.values())], axis=1)\n",
        "    if ec.dtype==tf.float32:\n",
        "        return features, ec\n",
        "    else:\n",
        "        return features, tf.strings.to_number(ec) if not tf.reduce_any(ec== b'') else tf.convert_to_tensor([float('NaN')],dtype=tf.float32)\n",
        "\n",
        "def apply_window(dataset, window_size, batch_size,training_set_ratio=0.8):\n",
        "    windowed_dataset = dataset.map(pack_features_vector)\n",
        "    windowed_dataset = windowed_dataset.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices((x, y)))\n",
        "    filter_nan_in_ec = lambda x, y: not tf.reduce_any(tf.math.logical_or(tf.math.is_nan(y),tf.math.less_equal(y,tf.constant([0],dtype=tf.float32))))\n",
        "    windowed_dataset = windowed_dataset.filter(filter_nan_in_ec)\n",
        "\n",
        "    num_samples = 0\n",
        "    for _ in windowed_dataset:\n",
        "        num_samples += 1\n",
        "    train_size = int(training_set_ratio*num_samples)\n",
        "    if num_samples - train_size < window_size:\n",
        "        print('Available dataset size (%d training, %d test) smaller than window size (%d), will skip' % (train_size, num_samples - train_size,window_size))\n",
        "        return None, None, 0, 0\n",
        "\n",
        "    windowed_dataset = windowed_dataset.window(window_size, shift=1, drop_remainder=True)\n",
        "    windowed_dataset = windowed_dataset.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(window_size), y)))\n",
        " \n",
        "    windowed_trainset = windowed_dataset.take(train_size)\n",
        "    windowed_testset = windowed_dataset.skip(train_size)\n",
        "\n",
        "    return windowed_trainset.batch(batch_size).repeat(),\\\n",
        "        windowed_testset.batch(batch_size).repeat(),\\\n",
        "        train_size,\\\n",
        "        num_samples-train_size"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CX1V_LHDAXi"
      },
      "source": [
        "def load_and_window_dataset(data_files, input_variables, output_variable, window_size,batch_size,training_set_ratio=0.8):\n",
        "    assert len(output_variable)==1, 'This script is for single-station estimation!'\n",
        "\n",
        "    # 1282 input variables, 25 salinity (output) stations\n",
        "    ec_csv_ds = tf.data.experimental.make_csv_dataset(\n",
        "        data_files,\n",
        "        batch_size=1,\n",
        "        na_value='-2.000', # missing entries in output\n",
        "        select_columns=input_variables+output_variable,\n",
        "        label_name=output_variable[0],# work on first output station\n",
        "        num_epochs=1,\n",
        "        ignore_errors=False,\n",
        "        shuffle=False)\n",
        "\n",
        "    windowed_trainset,windowed_testset,train_size,test_size = apply_window(ec_csv_ds, window_size, batch_size,training_set_ratio)\n",
        "\n",
        "    #     for feature, label in windowed_testset.take(1):\n",
        "    #         print('%s windowed feature shape: ' % (output_variable[0]), feature.shape, 'Label shape: ', label.shape)\n",
        "\n",
        "    return windowed_trainset, windowed_testset, train_size,test_size\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IxXjdKtHvon"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZMKHa5laLDq"
      },
      "source": [
        "# function that generates manual weights for first conv layer\n",
        "def conv_filter_generator(num_daily_values=7,avg_window_size = 11, num_windows=10):\n",
        "    w = np.zeros((1,num_daily_values+num_windows*avg_window_size,num_daily_values+num_windows))\n",
        "    for ii in range(num_daily_values):\n",
        "        w[0,num_daily_values+num_windows*avg_window_size-ii-1,num_daily_values-ii-1] = 1\n",
        "    for ii in range(num_windows):\n",
        "        w[0,((num_windows-ii-1)*avg_window_size):((num_windows-ii)*avg_window_size),num_daily_values+ii] = 1/avg_window_size\n",
        "    return w\n",
        "\n",
        "# weights to initialize the first conv layer\n",
        "conv_filter_init = tf.keras.initializers.Constant(conv_filter_generator(num_daily_values=num_daily_values,\n",
        "                                                                avg_window_size=avg_window_size,\n",
        "                                                                num_windows=num_windows))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4w6fZQjHtip"
      },
      "source": [
        "def build_ann(input_shape,num_daily_values, num_windows, conv_filter_init=None,train_conv=True,print_details=False):\n",
        "    conv_layer = tf.keras.layers.Conv1D(num_daily_values+num_windows, 1, activation=None,\n",
        "                              kernel_initializer=conv_filter_init,\n",
        "                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0))\n",
        "    conv_layer.trainable=train_conv\n",
        "    \n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Permute((2,1))(inputs)\n",
        "    x = tf.keras.layers.Masking(mask_value=-1)(x)\n",
        "    x = conv_layer(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(10, activation=\"elu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(10, activation=\"elu\")(x)\n",
        "    # x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "    outputs = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"ann_model\")\n",
        "    if print_details:\n",
        "        model.summary()\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rCYZvpxFDXY"
      },
      "source": [
        "## Define Callback functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni3OtFlyFE7G"
      },
      "source": [
        "### Learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kjaz9UFLAY"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 40%, 60%, 80%, 90% of total epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = initial_lr\n",
        "    if epoch > 0.9*epochs:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 0.8*epochs:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 0.6*epochs:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 0.4*epochs:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "  \n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxdt7MloFY2C"
      },
      "source": [
        "### Learning rate reducer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSL3JW-BFUbD"
      },
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                                                 cooldown=0,\n",
        "                                                 patience=5,\n",
        "                                                 min_lr=initial_lr/100)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdA8tViTF_Fu"
      },
      "source": [
        "### Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRoIW3Z8GCKi"
      },
      "source": [
        "def mse_loss_masked(y_true, y_pred):\n",
        "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred,y_true))\n",
        "    return squared_diff/(tf.reduce_sum(tf.cast(y_true>0, tf.float32))+0.01)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cOyl8r7HxdP"
      },
      "source": [
        "# Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1UjfI3sIXHD",
        "outputId": "7a9d92d2-ca4c-4ad6-fa6e-1a9b11de5c46"
      },
      "source": [
        "losses = {}\n",
        "val_losses = {}\n",
        "ann_number = 0\n",
        "for ii in range(len(output_variables)):\n",
        "    if output_variables[ii].replace('target','') not in key_stations:\n",
        "        pass\n",
        "    ann_number += 1\n",
        "    print('Training ANN #%d for Station %s' % (ann_number, output_variables[ii].replace('target','')))\n",
        "    \n",
        "    path_checkpoint = os.path.join('/content/drive','My Drive',google_drive_dir,model_folder,\"%s_checkpoint.h5\" % (output_variables[ii].replace('target','') + ('_train_conv' if train_conv else '_freeze_conv')))\n",
        "    # es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
        "    \n",
        "    modelckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        filepath=path_checkpoint,\n",
        "        verbose=1,\n",
        "        save_weights_only=True,\n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    # prepare dataset\n",
        "    selected_output_variable = output_variables[ii] # one output station at a time\n",
        "    windowed_trainset, windowed_testset, train_size, test_size = load_and_window_dataset(data_files, \n",
        "                                                                                         selected_input_variables,\n",
        "                                                                                         [selected_output_variable],\n",
        "                                                                                         window_size,\n",
        "                                                                                         batch_size,\n",
        "                                                                                         training_set_ratio)\n",
        "    if train_size == 0 or test_size == 0:\n",
        "        continue\n",
        "\n",
        "    # build new model\n",
        "    model = build_ann(input_shape=(window_size, len(selected_input_variables)),\n",
        "                      num_daily_values=num_daily_values,\n",
        "                      num_windows=num_windows,\n",
        "                      conv_filter_init=conv_filter_init,\n",
        "                      train_conv=train_conv)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "    \n",
        "    model.compile(\n",
        "        loss=mse_loss_masked,\n",
        "        optimizer=optimizer,\n",
        "        metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "    history = model.fit(\n",
        "      windowed_trainset,\n",
        "      epochs=epochs,\n",
        "      steps_per_epoch=train_size//batch_size,\n",
        "      validation_data=windowed_testset,\n",
        "      validation_freq=1,\n",
        "      validation_steps=test_size//batch_size,\n",
        "      callbacks=[lr_scheduler,modelckpt_callback],\n",
        "      verbose=1\n",
        "    )\n",
        "    losses[selected_output_variable] = history.history['loss'][-1]\n",
        "    val_losses[selected_output_variable] = history.history['val_loss'][-1]\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ANN #1 for Station CHDMC006\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.1124 - mean_squared_error: 0.1124\n",
            "Epoch 00001: val_loss improved from inf to 0.14037, saving model to /content/drive/My Drive/DeltaModelling/saved_models/CHDMC006_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 40s 669ms/step - loss: 0.1124 - mean_squared_error: 0.1124 - val_loss: 0.1404 - val_mean_squared_error: 0.1404 - lr: 0.0100\n",
            "Training ANN #2 for Station CHSWP003\n",
            "Learning rate:  0.01\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.0682 - mean_squared_error: 0.0682\n",
            "Epoch 00001: val_loss improved from inf to 0.05419, saving model to /content/drive/My Drive/DeltaModelling/saved_models/CHSWP003_train_conv_checkpoint.h5\n",
            "54/54 [==============================] - 41s 722ms/step - loss: 0.0682 - mean_squared_error: 0.0682 - val_loss: 0.0542 - val_mean_squared_error: 0.0542 - lr: 0.0100\n",
            "Training ANN #3 for Station CHVCT000\n",
            "Learning rate:  0.01\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2760 - mean_squared_error: 0.2760\n",
            "Epoch 00001: val_loss improved from inf to 2.08800, saving model to /content/drive/My Drive/DeltaModelling/saved_models/CHVCT000_train_conv_checkpoint.h5\n",
            "36/36 [==============================] - 56s 1s/step - loss: 0.2760 - mean_squared_error: 0.2760 - val_loss: 2.0880 - val_mean_squared_error: 2.0882 - lr: 0.0100\n",
            "Training ANN #4 for Station OLD_MID\n",
            "Learning rate:  0.01\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1545 - mean_squared_error: 0.1545\n",
            "Epoch 00001: val_loss improved from inf to 0.06864, saving model to /content/drive/My Drive/DeltaModelling/saved_models/OLD_MID_train_conv_checkpoint.h5\n",
            "58/58 [==============================] - 41s 693ms/step - loss: 0.1545 - mean_squared_error: 0.1545 - val_loss: 0.0686 - val_mean_squared_error: 0.0686 - lr: 0.0100\n",
            "Training ANN #5 for Station ROLD024\n",
            "Learning rate:  0.01\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.1176 - mean_squared_error: 0.1176\n",
            "Epoch 00001: val_loss improved from inf to 0.18331, saving model to /content/drive/My Drive/DeltaModelling/saved_models/ROLD024_train_conv_checkpoint.h5\n",
            "56/56 [==============================] - 57s 1s/step - loss: 0.1176 - mean_squared_error: 0.1176 - val_loss: 0.1833 - val_mean_squared_error: 0.1833 - lr: 0.0100\n",
            "Training ANN #6 for Station ROLD059\n",
            "Learning rate:  0.01\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.2461 - mean_squared_error: 0.2461\n",
            "Epoch 00001: val_loss improved from inf to 0.21902, saving model to /content/drive/My Drive/DeltaModelling/saved_models/ROLD059_train_conv_checkpoint.h5\n",
            "43/43 [==============================] - 40s 822ms/step - loss: 0.2461 - mean_squared_error: 0.2461 - val_loss: 0.2190 - val_mean_squared_error: 0.2190 - lr: 0.0100\n",
            "Training ANN #7 for Station RSAC064\n",
            "Learning rate:  0.01\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.1036 - mean_squared_error: 0.1036\n",
            "Epoch 00001: val_loss improved from inf to 0.95301, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAC064_train_conv_checkpoint.h5\n",
            "56/56 [==============================] - 42s 729ms/step - loss: 0.1036 - mean_squared_error: 0.1036 - val_loss: 0.9530 - val_mean_squared_error: 0.9531 - lr: 0.0100\n",
            "Training ANN #8 for Station RSAC075\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.1423 - mean_squared_error: 0.1423\n",
            "Epoch 00001: val_loss improved from inf to 0.13726, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAC075_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 41s 697ms/step - loss: 0.1423 - mean_squared_error: 0.1423 - val_loss: 0.1373 - val_mean_squared_error: 0.1373 - lr: 0.0100\n",
            "Training ANN #9 for Station RSAC081\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.1986 - mean_squared_error: 0.1986\n",
            "Epoch 00001: val_loss improved from inf to 0.05799, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAC081_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 41s 707ms/step - loss: 0.1986 - mean_squared_error: 0.1986 - val_loss: 0.0580 - val_mean_squared_error: 0.0580 - lr: 0.0100\n",
            "Training ANN #10 for Station RSAC092\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
            "Epoch 00001: val_loss improved from inf to 0.05391, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAC092_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 41s 705ms/step - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0539 - val_mean_squared_error: 0.0539 - lr: 0.0100\n",
            "Training ANN #11 for Station RSAC101\n",
            "Learning rate:  0.01\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.1286 - mean_squared_error: 0.1286\n",
            "Epoch 00001: val_loss improved from inf to 0.03379, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAC101_train_conv_checkpoint.h5\n",
            "47/47 [==============================] - 40s 772ms/step - loss: 0.1286 - mean_squared_error: 0.1286 - val_loss: 0.0338 - val_mean_squared_error: 0.0338 - lr: 0.0100\n",
            "Training ANN #12 for Station RSAN007\n",
            "Learning rate:  0.01\n",
            "53/53 [==============================] - ETA: 0s - loss: 0.3760 - mean_squared_error: 0.3760\n",
            "Epoch 00001: val_loss improved from inf to 9.63587, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAN007_train_conv_checkpoint.h5\n",
            "53/53 [==============================] - 41s 753ms/step - loss: 0.3760 - mean_squared_error: 0.3760 - val_loss: 9.6359 - val_mean_squared_error: 9.6369 - lr: 0.0100\n",
            "Training ANN #13 for Station RSAN018\n",
            "Learning rate:  0.01\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1126 - mean_squared_error: 0.1126\n",
            "Epoch 00001: val_loss improved from inf to 0.10739, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAN018_train_conv_checkpoint.h5\n",
            "58/58 [==============================] - 42s 697ms/step - loss: 0.1126 - mean_squared_error: 0.1126 - val_loss: 0.1074 - val_mean_squared_error: 0.1074 - lr: 0.0100\n",
            "Training ANN #14 for Station RSAN032\n",
            "Learning rate:  0.01\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.0780 - mean_squared_error: 0.0780\n",
            "Epoch 00001: val_loss improved from inf to 0.81832, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAN032_train_conv_checkpoint.h5\n",
            "57/57 [==============================] - 40s 688ms/step - loss: 0.0780 - mean_squared_error: 0.0780 - val_loss: 0.8183 - val_mean_squared_error: 0.8184 - lr: 0.0100\n",
            "Training ANN #15 for Station RSAN037\n",
            "Learning rate:  0.01\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.1024 - mean_squared_error: 0.1024\n",
            "Epoch 00001: val_loss improved from inf to 0.32061, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAN037_train_conv_checkpoint.h5\n",
            "32/32 [==============================] - 54s 1s/step - loss: 0.1024 - mean_squared_error: 0.1024 - val_loss: 0.3206 - val_mean_squared_error: 0.3206 - lr: 0.0100\n",
            "Training ANN #16 for Station RSAN058\n",
            "Learning rate:  0.01\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2355 - mean_squared_error: 0.2355\n",
            "Epoch 00001: val_loss improved from inf to 1.45541, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAN058_train_conv_checkpoint.h5\n",
            "55/55 [==============================] - 56s 997ms/step - loss: 0.2355 - mean_squared_error: 0.2355 - val_loss: 1.4554 - val_mean_squared_error: 1.4556 - lr: 0.0100\n",
            "Training ANN #17 for Station RSAN072\n",
            "Learning rate:  0.01\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.1638 - mean_squared_error: 0.1638\n",
            "Epoch 00001: val_loss improved from inf to 0.15522, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSAN072_train_conv_checkpoint.h5\n",
            "43/43 [==============================] - 39s 801ms/step - loss: 0.1638 - mean_squared_error: 0.1638 - val_loss: 0.1552 - val_mean_squared_error: 0.1552 - lr: 0.0100\n",
            "Training ANN #18 for Station RSMKL008\n",
            "Learning rate:  0.01\n",
            "58/58 [==============================] - ETA: 0s - loss: 0.1648 - mean_squared_error: 0.1648\n",
            "Epoch 00001: val_loss improved from inf to 0.32964, saving model to /content/drive/My Drive/DeltaModelling/saved_models/RSMKL008_train_conv_checkpoint.h5\n",
            "58/58 [==============================] - 42s 708ms/step - loss: 0.1648 - mean_squared_error: 0.1648 - val_loss: 0.3296 - val_mean_squared_error: 0.3297 - lr: 0.0100\n",
            "Training ANN #19 for Station SLCBN002\n",
            "Learning rate:  0.01\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.0851 - mean_squared_error: 0.0851\n",
            "Epoch 00001: val_loss improved from inf to 0.96922, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SLCBN002_train_conv_checkpoint.h5\n",
            "36/36 [==============================] - 54s 1s/step - loss: 0.0851 - mean_squared_error: 0.0851 - val_loss: 0.9692 - val_mean_squared_error: 0.9693 - lr: 0.0100\n",
            "Training ANN #20 for Station SLDUT007\n",
            "Learning rate:  0.01\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.2385 - mean_squared_error: 0.2385\n",
            "Epoch 00001: val_loss improved from inf to 0.20617, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SLDUT007_train_conv_checkpoint.h5\n",
            "29/29 [==============================] - 38s 1s/step - loss: 0.2385 - mean_squared_error: 0.2385 - val_loss: 0.2062 - val_mean_squared_error: 0.2062 - lr: 0.0100\n",
            "Training ANN #21 for Station SLMZU011\n",
            "Learning rate:  0.01\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.2218 - mean_squared_error: 0.2218\n",
            "Epoch 00001: val_loss improved from inf to 0.04094, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SLMZU011_train_conv_checkpoint.h5\n",
            "56/56 [==============================] - 55s 971ms/step - loss: 0.2218 - mean_squared_error: 0.2218 - val_loss: 0.0409 - val_mean_squared_error: 0.0409 - lr: 0.0100\n",
            "Training ANN #22 for Station SLMZU025\n",
            "Learning rate:  0.01\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.5457 - mean_squared_error: 0.5458\n",
            "Epoch 00001: val_loss improved from inf to 0.49874, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SLMZU025_train_conv_checkpoint.h5\n",
            "42/42 [==============================] - 38s 799ms/step - loss: 0.5457 - mean_squared_error: 0.5458 - val_loss: 0.4987 - val_mean_squared_error: 0.4988 - lr: 0.0100\n",
            "Training ANN #23 for Station SLSUS012\n",
            "Learning rate:  0.01\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1645 - mean_squared_error: 0.1645\n",
            "Epoch 00001: val_loss improved from inf to 2.73785, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SLSUS012_train_conv_checkpoint.h5\n",
            "34/34 [==============================] - 37s 905ms/step - loss: 0.1645 - mean_squared_error: 0.1645 - val_loss: 2.7378 - val_mean_squared_error: 2.7381 - lr: 0.0100\n",
            "Training ANN #24 for Station SLTRM004\n",
            "Learning rate:  0.01\n",
            "33/33 [==============================] - ETA: 0s - loss: 0.1198 - mean_squared_error: 0.1198\n",
            "Epoch 00001: val_loss improved from inf to 0.06648, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SLTRM004_train_conv_checkpoint.h5\n",
            "33/33 [==============================] - 39s 955ms/step - loss: 0.1198 - mean_squared_error: 0.1198 - val_loss: 0.0665 - val_mean_squared_error: 0.0665 - lr: 0.0100\n",
            "Training ANN #25 for Station SSS\n",
            "Learning rate:  0.01\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0977 - mean_squared_error: 1.0978\n",
            "Epoch 00001: val_loss improved from inf to 137.01077, saving model to /content/drive/My Drive/DeltaModelling/saved_models/SSS_train_conv_checkpoint.h5\n",
            "5/5 [==============================] - 33s 5s/step - loss: 1.0977 - mean_squared_error: 1.0978 - val_loss: 137.0108 - val_mean_squared_error: 137.0703 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_n-mdjPTHPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8d7c4b-7d4e-42a1-e311-9437dc1daf0b"
      },
      "source": [
        "print ((\"{:<15}\" * (3)).format('', 'Loss', 'Val Loss'))\n",
        "\n",
        "for station, loss in losses.items():\n",
        "    print((\"{:<15}\" * (3)).format(station.replace('target',''), np.round(loss,8), np.round(val_losses[station],8)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Loss           Val Loss       \n",
            "CHDMC006       0.11239391     0.14037049     \n",
            "CHSWP003       0.06823615     0.05418694     \n",
            "CHVCT000       0.27601081     2.08799958     \n",
            "OLD_MID        0.15451789     0.06863814     \n",
            "ROLD024        0.1176272      0.18330817     \n",
            "ROLD059        0.24607235     0.21902433     \n",
            "RSAC064        0.10359459     0.95300621     \n",
            "RSAC075        0.14228572     0.13725588     \n",
            "RSAC081        0.1986206      0.05799089     \n",
            "RSAC092        0.03310212     0.05390758     \n",
            "RSAC101        0.12860398     0.03379159     \n",
            "RSAN007        0.3759791      9.63587093     \n",
            "RSAN018        0.11260133     0.10739285     \n",
            "RSAN032        0.07800633     0.81832427     \n",
            "RSAN037        0.10235355     0.32061225     \n",
            "RSAN058        0.23551987     1.45540702     \n",
            "RSAN072        0.16382222     0.155222       \n",
            "RSMKL008       0.16483136     0.32964095     \n",
            "SLCBN002       0.0850665      0.96922451     \n",
            "SLDUT007       0.23848146     0.20616595     \n",
            "SLMZU011       0.22181927     0.04093648     \n",
            "SLMZU025       0.54573202     0.49874467     \n",
            "SLSUS012       0.16449901     2.73784685     \n",
            "SLTRM004       0.11978526     0.06647807     \n",
            "SSS            1.09770286     137.01077271   \n"
          ]
        }
      ]
    }
  ]
}