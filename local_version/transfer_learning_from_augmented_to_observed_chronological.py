# -*- coding: utf-8 -*-
"""Transfer_Learning_from_Augmented_to_Observed_Chronological.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tptKxcC9e-_P3yrzh2m8qz-gb7uY8lV-

# Transfer Learning to Observed Data

## Define hyper-parameters
"""

####################################################
############## Hyper-param definitions #############
####################################################

local_root_path = '/Users/siyuqi/Downloads/DWR_Workshop'

''' 
Select NN architecture from:
MLP, LSTM, GRU, ResNet, Residual_LSTM, Residual_GRU, Transformer
'''
model_type ='Residual_LSTM'

''' 
Name of pre-trained models (to be transferred)
'''
pretrain_model_path_prefix='mtl_i118_residual_lstm_8_2_Tune_RSAC_RSAN'

''' 
Numbers of neurons in the main branch
 - Provide two numbers for MLP, ResNet, Res-LSTM, Res-GRU, 
 - Provide one number for LSTM, GRU.
 '''
num_neurons_multiplier = [8,2]

'''
Number of training epochs (Note: training will stop when reaching this number 
or test loss doesn't decrease for 50 epochs)
'''
epochs = 5000

''' 
Whether to (True) train models from scratch or (False) evaluate pre-trained models
'''
train_models = True

''' 
Whether to augment inputs (jittering and dropout)
'''
augmentation = False

''' 
Training and test set, in years
'''
calib_slice = slice('2000', '2016')
valid_slice = slice('2017', '2020')

''' 
Learning rate multiplier
Defined as ratio of learning rate between front layers and output layer
'''
lr_mult=0.1

''' 
Model resolution
- '1D': daily
- '1h': hourly
'''
interval = '1D'

''' 
Salinity forecasting lead time, unit is given by 'interval'
'''
lead_time = 0

####################################################
########## End of hyper-param definitions ##########
####################################################

import re

lead_freq = re.split('(\d+)',interval)[-1]

if lead_time > 0:
    forecast_marker = '_forecast_%dx%s' % (lead_time,interval)
else:
    forecast_marker = ''

model_type = model_type.lower()
assert (model_type in pretrain_model_path_prefix) or (model_type=='mlp' and 'd' in pretrain_model_path_prefix) or (model_type=='gru' and 'g' in pretrain_model_path_prefix)
import os
import sys
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt


given_name = '%s%s%s_hist' % (model_type, (('_'+interval) if interval.lower() != '1d' else ''), forecast_marker)



if augmentation:
    noise_sigma=0.03
    dropout_ratio = 0.05
else:
    noise_sigma=0.
    dropout_ratio = 0.

group_stations = False
initial_lr = 0.001
data_file = "observed_data_daily.xlsx"

print('Dataset: %s' % data_file)

data_path = os.path.join(local_root_path,data_file)
num_sheets = {"observed_data_daily.xlsx":9,
              "observed_data_hourly.xlsx":9}
sys.path.append(local_root_path)


'''
Define parameters for input pre-processing
- ndays: number of daily values in inputs
- window_size: length of averaging windows
- nwindows: number of moving averages
'''
if model_type =='mlp':
    # apply pre-defined average windowing:
    ndays=8
    window_size=11
    nwindows=10
else:
    # directly use daily measurements as inputs
    ndays=118
    window_size=0
    nwindows=0

# percentile thresholds for ranged results
percentiles = [0,0.75,0.95]  
model_size = 'x'.join([str(n) for n in num_neurons_multiplier if n > 0])

"""## Install Packages"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras import layers
#import keras
from collections import defaultdict
import joblib
import time

!pip install tensorflow-addons
import tensorflow_addons as tfa

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score

"""## Read Data"""

dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets[data_file])]
def load_data(data_file):
    if data_file in ["dsm2_ann_observed_1h.xlsx","dsm2_ann_observed_15min.xlsx"]:
        dfouts=dflist[-1]
        df2 = pd.pivot_table(dfouts, index=dfouts.index.date, columns=dfouts.index.hour)
        df2.columns=df2.columns.get_level_values(0)
        output_col_list = df2.columns
        df_inpout = pd.concat(dflist[0:(num_sheets[data_file]-1)]+[df2],axis=1).dropna(axis=0)
        dfinps = df_inpout.loc[:,~df_inpout.columns.isin(output_col_list)]
        dfouts = df_inpout.loc[:,df_inpout.columns.isin(output_col_list)]
    else:
        df_inpout = pd.concat(dflist[0:(num_sheets[data_file])],axis=1).dropna(axis=0)
        dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets[data_file]-1].columns)]
        dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets[data_file]-1].columns)]
    return dfinps, dfouts

dfinps, dfouts = load_data(data_file)
dfinps.head()

dfouts

output_locations_w_duplicates_for_time = dfouts.columns[~dfouts.columns.str.contains('_dup')]
output_locations = set(output_locations_w_duplicates_for_time)
print('Found %d stations' % len(output_locations))

"""## Model Definitions"""

"""# Custom loss function"""
def mse_loss_masked(y_true, y_pred):
    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred[y_true>=0],y_true[y_true>=0]))
    return squared_diff/(tf.reduce_sum(tf.cast(y_true>=0, tf.float32))+0.01)

# Define Sequential model
# LSTM model has one LSTM layer
# MLP model has 3 Dense layers
NFEATURES = (ndays + nwindows) * (num_sheets[data_file]-1)

def build_lstm_model(lstm_units=8,lstm_units2=None, output_shape=1, act_func='sigmoid',layer_type='lstm'):
    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU
    model = keras.Sequential(
        [
            layers.Input(shape=(NFEATURES)),
            layers.Reshape((ndays+nwindows,num_sheets[data_file]-1)),
            layers.Permute((2,1)),
            rnn_layer(units = lstm_units, activation=act_func,return_sequences=True),
            # rnn_layer(units = lstm_units, activation=act_func),
            layers.Flatten(),
            layers.Dense(output_shape, activation=keras.activations.linear,name='output')
        ])
    model.compile(optimizer=keras.optimizers.Adam(
        learning_rate=initial_lr), loss=mse_loss_masked)
    return model

def build_residual_lstm_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',layer_type='lstm',conv_init=None):
    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU
    inputs = layers.Input(shape=NFEATURES)
    x = layers.Reshape((ndays+nwindows,num_sheets[data_file]-1))(inputs)
    x = layers.Permute((2,1))(x)

    y = tf.keras.layers.Conv1D(8+10,1, activation='relu',
                            kernel_initializer=conv_init,
                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),
                            trainable=False)(x)

    y = layers.Flatten()(y)
    y = layers.Dense(nhidden1, activation=act_func)(y)
    y = layers.Dense(nhidden2, activation=act_func)(y)
    y = layers.Dense(output_shape, activation=keras.activations.linear,name='mlp_output')(y)

    shortcut = x
    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)
    shortcut = rnn_layer(units = output_shape*2, activation=act_func,return_sequences=True)(shortcut)
    shortcut = layers.Flatten()(shortcut)
    shortcut = layers.Dense(output_shape, activation=keras.activations.linear,name='lstm_output')(shortcut)

    outputs = layers.Add(name="res_add")([y, shortcut])
    # outputs = layers.Activation("relu",name="res_relu")(outputs)
    outputs = layers.LeakyReLU(alpha=0.3,name="res_relu")(outputs)
    
    model = keras.Model(inputs=inputs, outputs=outputs)

    model.compile(optimizer=keras.optimizers.Adam(
        learning_rate=initial_lr), loss=mse_loss_masked)
    return model



def build_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid'):
    model = keras.Sequential(
        [
            layers.Input(shape=(NFEATURES)),
            layers.Dense(nhidden1, activation=act_func),
            layers.Dense(nhidden2, activation=act_func),
            layers.Dense(output_shape, activation=keras.activations.linear,name='output')
        ])
    model.compile(optimizer=keras.optimizers.Adam(
        learning_rate=initial_lr), loss=mse_loss_masked)
    #model.compile(optimizer=keras.optimizers.RMSprop(), loss=mse_loss_masked)
    return model

def build_resnet_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',filters=num_sheets[data_file]-1, kernel_size=3, stride=1):
    inputs = layers.Input(shape=NFEATURES)
    x = layers.Reshape((ndays+nwindows,num_sheets[data_file]-1))(inputs)

    y = layers.ZeroPadding1D(padding=1,name="padding_branch2a")(x)
    y = layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False,
                                name="res_branch2a")(y)
    y = layers.BatchNormalization()(y)
    y = layers.Activation("relu", name="res_branch2a_relu")(y)

    y = layers.ZeroPadding1D(padding=1,name="padding_branch2b")(y)
    y = layers.Conv1D(filters,kernel_size,use_bias=False,
                            name="res_branch2b")(y)
    y = layers.BatchNormalization()(y)
    y = layers.Flatten()(y)
    y = layers.Dense(nhidden1, activation=act_func)(y)

    shortcut = inputs
    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)

    y = layers.Add(name="res")([y, shortcut])
    
    y = layers.Activation("relu",name="res_relu")(y)


    y = layers.Dense(nhidden2, activation=act_func)(y)
    outputs= layers.Dense(output_shape, activation=keras.activations.linear,name='output')(y)

    model = keras.Model(inputs=inputs, outputs=outputs)

    model.compile(optimizer=keras.optimizers.Adam(
        learning_rate=initial_lr), loss=mse_loss_masked)
    return model

"""## Evaluation Metrics"""

eval_metrics = ['MSE', 'Bias', 'R', 'RMSD', 'NSE']
key_stations = ['RSAC064','CCWD_Rock','CHDMC006', 'CHSWP003','RSAC092','RSAN018']

def evaluate_sequences(target, pred, metrics):
    assert len(target) == len(pred), 'Target and predicted sequence length must equal.'
    if len(target.shape)==1 or target.shape[-1]==1:                                     
        valid_entries = target>=0
    else:
        valid_entries = (target>=0).all(axis=1)
    sequence_length = np.sum(valid_entries)
    if np.any(sequence_length == 0):
        return {k: 0 for k in metrics}
    target=target[valid_entries]
    pred = pred[valid_entries]
    SD_pred = np.sqrt( np.sum((pred-np.mean(pred)) ** 2) /(sequence_length-1))
    SD_target = np.sqrt( np.sum((target-np.mean(target)) ** 2) /(sequence_length-1))

    eval_results = defaultdict(float)
    
    for m in metrics:
        if m =='MSE':
            eval_results[m] = ((target - pred)**2).mean()
        elif m =='Bias':
            eval_results[m] = np.sum(pred - target)/np.sum(target) * 100
        elif m == 'R':
            eval_results[m] = np.sum(np.abs((pred-np.mean(pred)) * (target - np.mean(target)))) / (sequence_length * SD_pred * SD_target)
        elif m == 'RMSD':
            eval_results[m] = np.sqrt(np.sum( ( ( pred-np.mean(pred) ) * ( target - np.mean(target) ) ) ** 2 ) / sequence_length)
        elif m == 'NSE':
            eval_results[m] = 1 - np.sum( ( target - pred ) ** 2 ) / np.sum( (target - np.mean(target) ) ** 2 )
    return eval_results

import annutils
result_folders = ['models','results','images']
for result_folder in result_folders:
    if not os.path.exists(os.path.join(local_root_path, result_folder)):
        os.makedirs(os.path.join(local_root_path, result_folder))

station_without_groups = {'all':output_locations}
station_with_groups = {'G1':['SSS','RSAC101','RSMKL008'],
                  'G2':['Old_River_Hwy_4','Middle_River_Intake','CCWD_Rock','SLTRM004','RSAN032','RSAN037','SLDUT007','ROLD024','RSAN058','RSAN072','OLD_MID','ROLD059','CHDMC006','CHSWP003','CHVCT000'],
                  'G3':['SLCBN002','SLSUS012','SLMZU011','SLMZU025','RSAC064','RSAC075','RSAC081','RSAN007','RSAC092','RSAN018','Martinez']}
final_groups = {False: station_without_groups, 
                True: station_with_groups}

"""## Load Pre-Trained Models and Apply Transfer Learning"""

for group_name, stations in final_groups[group_stations].items():
    if not train_models:
        break
    selected_output_variables = []
    for station in stations:
        for output in output_locations:
            if station in output:
                selected_output_variables.append(output)


    print('Training MTL ANN for %d stations: ' % len(selected_output_variables))
    print([station.replace('target/','').replace('target','') for station in selected_output_variables])
    

    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs
    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \
        annutils.create_training_sets([dfinps],
                                      [dfouts[selected_output_variables]],
                                      #train_frac=train_frac,
                                      calib_slice=calib_slice,
                                      valid_slice=valid_slice,
                                      ndays=ndays,window_size=window_size,nwindows=nwindows,
                                      noise_sigma=noise_sigma,dropout_ratio=dropout_ratio,
                                      lead_time=lead_time,lead_freq=lead_freq)
    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:
        try:
            loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={"mse_loss_masked": mse_loss_masked})
            pretrained_model = loaded_model.model
            print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))
        except:
            # only model weights are saved, build model then load
            if model_type.lower() in ['lstm','gru']:
                pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),
                                                    num_neurons_multiplier[1]*len(output_locations),
                                                    output_shape=yallc.shape[1],
                                                    act_func='sigmoid',
                                                    layer_type=model_type.lower())
            elif model_type.lower() == 'mlp':
                pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),
                                               num_neurons_multiplier[1]*len(output_locations),
                                               output_shape=yallc.shape[1], act_func='sigmoid')
            elif model_type.lower() =='resnet':
                pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),
                                                      num_neurons_multiplier[1]*len(output_locations),
                                                      output_shape=yallc.shape[1], act_func='sigmoid')
        
            pretrained_model.load_weights(os.path.join(local_root_path,'models', pretrain_model_path_prefix+'.h5'))
        
        print('Transfer learning from pre-trained model %s' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))
        o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)
        model = keras.Model(inputs=pretrained_model.input, outputs=[o])

        
        optimizers = [
            tf.keras.optimizers.Adam(learning_rate=lr_mult*initial_lr),
            tf.keras.optimizers.Adam(learning_rate=initial_lr)
        ]
        optimizers_and_layers = [(optimizers[0], model.layers[0:-1]), (optimizers[1], model.layers[-1])]

        model.compile(optimizer=tfa.optimizers.MultiOptimizer(optimizers_and_layers),
                      loss=mse_loss_masked)
    elif model_type.lower() in ['lstm','gru']:
        model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),
                                 num_neurons_multiplier[1]*len(output_locations),
                                 output_shape=yallc.shape[1],
                                 act_func='sigmoid',
                                 layer_type=model_type.lower())
    elif model_type.lower() == 'mlp':
        model = build_model(num_neurons_multiplier[0]*len(output_locations),
                            num_neurons_multiplier[1]*len(output_locations),
                            output_shape=yallc.shape[1], act_func='sigmoid')
    elif model_type.lower() =='resnet':
        model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),
                                   num_neurons_multiplier[1]*len(output_locations),
                                   output_shape=yallc.shape[1], act_func='sigmoid')
    elif model_type.lower() in ['res-lstm','res-gru']:
        conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=8,window_size=11,nwindows=10))

        model = build_residual_lstm_model(num_neurons_multiplier[0]*len(output_locations),
                                 num_neurons_multiplier[1]*len(output_locations),
                                 output_shape=yallc.shape[1],
                                 act_func='sigmoid',
                                 layer_type=model_type.lower()[len('res-'):],
                                 conv_init=conv_init)

    model.summary()
    start_time = time.time()
    history = model.fit(
        xallc,
        yallc,
        epochs=epochs,
        batch_size=128,
        validation_data=(xallv, yallv),
        callbacks=[
            keras.callbacks.EarlyStopping(
                monitor="val_loss", patience=50, mode="min", restore_best_weights=True),
        ],
        verbose=0,
    )
    end_time=time.time()
    print('Training time: %d min' % ((end_time-start_time)/60) )
    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)
    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:
        joblib.dump((xscaler,yscaler),'%s_xyscaler.dump'%model_save_path)
        model.save_weights('%s.h5'%model_save_path)
    else:
        annutils.save_model(model_save_path, model, xscaler, yscaler)
    print('Model saved at %s.h5' % model_save_path)

"""# Evaluation

## Numerical Results
"""

full_results={}
range_results=defaultdict(defaultdict)

df_inpout = pd.concat(dflist[0:(num_sheets[data_file])],axis=1).dropna(axis=0)
for group_name, stations in final_groups[group_stations].items():
    # prepare dataset
    selected_output_variables = []
    for station in stations:
        for output in output_locations:
            if station in output:
                selected_output_variables.append(output)
    print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))

    print([station.replace('target/','').replace('target','') for station in selected_output_variables])

    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)

    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs
    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \
        annutils.create_training_sets([dfinps],
                                      [dfouts[selected_output_variables]],
                                      #train_frac=train_frac,
                                      calib_slice=calib_slice,
                                      valid_slice=valid_slice,
                                      ndays=ndays,window_size=window_size,nwindows=nwindows,
                                      lead_time=lead_time,lead_freq=lead_freq)
    try:
        annmodel = annutils.load_model(model_save_path,
                                       custom_objects={"mse_loss_masked": mse_loss_masked})
        model = annmodel.model
    except:
        print('Unable to load saved model, rebuilding from pre-trained model...')
        if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:
            try:
                loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={"mse_loss_masked": mse_loss_masked})
                pretrained_model = loaded_model.model
                print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))
            except:
                # only model weights are saved, build model then load
                if model_type.lower() in ['lstm','gru']:
                    pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),
                                                        num_neurons_multiplier[1]*len(output_locations),
                                                        output_shape=yallc.shape[1],
                                                        act_func='sigmoid',
                                                        layer_type=model_type.lower())
                elif model_type.lower() == 'mlp':
                    pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),
                                                   num_neurons_multiplier[1]*len(output_locations),
                                                   output_shape=yallc.shape[1], act_func='sigmoid')
                elif model_type.lower() =='resnet':
                    pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations), 
                                                          num_neurons_multiplier[1]*len(output_locations),
                                                          output_shape=yallc.shape[1], act_func='sigmoid')
            
            # remove last layer and attach new
            o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)
            model = keras.Model(inputs=pretrained_model.input, outputs=[o])

        model.load_weights(model_save_path+'.h5')
        print('Loaded model weights from %s.h5' % model_save_path)
        xscaler,yscaler=joblib.load('%s_xyscaler.dump'%model_save_path)


    train_pred = pd.DataFrame(np.clip(model.predict(xallc),0,1),yallc.index,columns=yallc.columns)
    test_pred = pd.DataFrame(np.clip(model.predict(xallv),0,1),yallv.index,columns=yallv.columns)

    for ii, location in enumerate(selected_output_variables):
        
        # compute training results
        train_results = evaluate_sequences(yallc.loc[:,location].to_numpy().reshape(-1,1),
                                           train_pred.loc[:,location].to_numpy().reshape(-1,1),
                                           eval_metrics)
        train_results['R^2'] = r2_score(train_pred.loc[:,location].to_numpy().reshape(-1,1),
                                        yallc.loc[:,location].to_numpy().reshape(-1,1))
        full_results['%s_train' %location] = train_results

        # compute evaluation results
        eval_results = evaluate_sequences(yallv.loc[:,location].to_numpy().reshape(-1,1),
                                          test_pred.loc[:,location].to_numpy().reshape(-1,1),
                                          eval_metrics)
        eval_results['R^2'] = r2_score(test_pred.loc[:,location].to_numpy().reshape(-1,1),
                                       yallv.loc[:,location].to_numpy().reshape(-1,1))

        full_results['%s_test' %location] = eval_results

        all_target = np.concatenate((yallc[location].to_numpy().reshape(-1,1),
                                    yallv[location].to_numpy().reshape(-1,1)),axis=0)
        all_pred = np.concatenate((train_pred[location].to_numpy().reshape(-1,1),
                                    test_pred[location].to_numpy().reshape(-1,1)),axis=0)

        # compute results at different EC ranges
        for (lower_quantile, upper_quantile) in zip(percentiles,percentiles[1:]+[1,]):
            lower_threshold = np.quantile(all_target, lower_quantile)
            upper_threshold = np.quantile(all_target, upper_quantile)
            eval_results = evaluate_sequences(all_target[(all_target > lower_threshold) & (all_target <= upper_threshold)],
                                              all_pred[(all_target > lower_threshold) & (all_target <= upper_threshold)],
                                              eval_metrics)
            range_results[location][lower_quantile*100] = eval_results
